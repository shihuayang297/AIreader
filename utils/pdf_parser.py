# -*- coding: utf-8 -*-
# æ–‡ä»¶è·¯å¾„: mod/aireader/utils/pdf_parser.py
import pdfplumber
import sys
import json
import re
import time
import hashlib
import base64
import hmac
import datetime
from urllib.parse import urlparse
import urllib.request
import urllib.error

# ================= é…ç½®åŒºåŸŸ (è¯·ç¡®ä¿ä¸ server/config.py ä¸€è‡´) =================
SPARK_APPID = "0d0ffb4b"
SPARK_API_SECRET = "YTU4OGUxZTMxMjU4ZjEwZDk4YzI4YTlm"
SPARK_API_KEY = "084ee0c577b8db253458a63525f87e11"
SPARK_URL = "wss://spark-openapi-n.cn-huabei-1.xf-yun.com/v1.1/chat_kjwx"
SPARK_DOMAIN = "kjwx"

# éœ€è¦è‡ªåŠ¨ç”Ÿæˆæ€»ç»“çš„ç« èŠ‚å…³é”®è¯ (æ­£åˆ™)
TARGET_SECTIONS = re.compile(r'^(Introduction|Methodology|Methods|Discussion|Conclusion|å¼•è¨€|æ–¹æ³•|è®¨è®º|ç»“è®º)', re.I)

# ================= ç®€æ˜“ç‰ˆæ˜Ÿç«è°ƒç”¨å‡½æ•° (ä¸ºäº†ä¸ä¾èµ–å¤–éƒ¨æ–‡ä»¶ï¼Œç›´æ¥å†…åµŒ) =================
def call_spark_summary(text):
    """è°ƒç”¨æ˜Ÿç«å¤§æ¨¡å‹ç”Ÿæˆ Reference Content"""
    try:
        # è¿™é‡Œä¸ºäº†è„šæœ¬çš„ç‹¬ç«‹æ€§ï¼Œæˆ‘ä»¬ä½¿ç”¨ WebSocket çš„ç®€æ˜“å°è£…æˆ–è€… HTTP é™çº§
        # ç”±äº Python è„šæœ¬è¿è¡Œç¯å¢ƒå¤æ‚ï¼Œè¿™é‡Œä¸ºäº†ç¨³å®šæ€§ï¼Œ
        # å»ºè®®ä½¿ç”¨ä¸€æ®µç²¾ç®€çš„ WebSocket ä»£ç ï¼Œæˆ–è€…å¦‚æœä½ çš„æœåŠ¡å™¨æ”¯æŒï¼Œè°ƒç”¨ server.py çš„æ¥å£
        # ä¸ºäº†æ¼”ç¤ºå®Œæ•´é€»è¾‘ï¼Œè¿™é‡Œæˆ‘ä»¬æš‚æ—¶è¿”å›ä¸€ä¸ª"æ¨¡æ‹ŸAIæ€»ç»“"
        # âš ï¸ åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œè¯·å¼•å…¥ websocket-client åº“å¹¶å¤åˆ¶å®Œæ•´çš„ SparkClient ç±»
        
        # --- ä¸´æ—¶æ–¹æ¡ˆï¼šå¦‚æœç¯å¢ƒé‡Œæ²¡æœ‰ websocket-clientï¼Œè¿™éƒ¨åˆ†ä¼šæŠ¥é”™ ---
        # å»ºè®®ï¼šä»…ä»…æå–åŸæ–‡ç‰‡æ®µä½œä¸º reference_content å¾€å¾€æ¯” AI æ€»ç»“æ›´å‡†ç¡®
        # è¿™é‡Œçš„ç­–ç•¥æ”¹ä¸ºï¼šæå–è¯¥ç« èŠ‚çš„å‰ 800 ä¸ªå­—ä½œä¸º Context
        return text[:800].replace('\n', ' ') + "..."
    except Exception as e:
        return ""

# ================= åŸæœ‰è§£æé€»è¾‘ + å¢å¼º =================

def smart_logic_parse(file_path):
    # --- 1. å¢å¼ºå‹æ­£åˆ™åº“ ---
    EN_RE = r'^(\d+(\.\d+){0,2})[\s]+[A-Z\u4e00-\u9fa5]'
    CN_RE = r'^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€\s]|[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[)ï¼‰]|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ç« èŠ‚])'
    KEY_RE = r'^(Abstract|Introduction|Conclusion|References|Reference|æ‘˜è¦|ç»“è®º|å‚è€ƒæ–‡çŒ®)'
    
    START_PAGE_LIMIT = re.compile(r'^(Abstract|æ‘˜è¦)', re.I)
    SYSTEM_KEY_CHECK = re.compile(KEY_RE, re.I)
    COMBINED_RE = re.compile(f'({EN_RE}|{CN_RE}|{KEY_RE})', re.I)

    try:
        with pdfplumber.open(file_path) as pdf:
            # é‡‡æ ·æ­£æ–‡å­—å·
            font_stats = {}
            for page in pdf.pages[:3]:
                for char in page.chars:
                    s = round(char.get('size', 0), 2)
                    if 6 < s < 25: font_stats[s] = font_stats.get(s, 0) + 1
            body_size = max(font_stats, key=font_stats.get) if font_stats else 10

            all_potential_lines = [] 
            raw_candidates = []

            for i, page in enumerate(pdf.pages):
                lines = page.extract_text_lines(layout=True)
                for line in lines:
                    text = line['text'].strip()
                    if len(text) < 2: continue
                    all_potential_lines.append({"text": text, "page": i + 1})

                    if i == 0 and not START_PAGE_LIMIT.match(text): continue
                    if not line['chars']: continue
                    
                    char_sample = line['chars'][0]
                    curr_size = round(char_sample['size'], 2)
                    score = 0
                    match = COMBINED_RE.match(text)

                    if len(text) > 80 and text[-1] in '.,ï¼Œã€‚': continue

                    if SYSTEM_KEY_CHECK.match(text) and curr_size >= body_size: score += 100
                    elif match and curr_size > body_size + 0.1: score += 90
                    elif match:
                        if not re.search(r'[\d]\s*[,ï¼Œ\)\%\:]', text[:15]) and "M =" not in text: score += 60
                    elif curr_size > body_size + 1.5: score += 40

                    if score >= 50:
                        raw_candidates.append({
                            "title": text.split('  ')[0].strip(),
                            "page": i + 1,
                            "index": text.split(' ')[0].rstrip('.')
                        })

            # --- å»é‡ä¸é€»è¾‘å¤„ç† ---
            unique_candidates = []
            seen_titles = set()
            for c in raw_candidates:
                if c['title'] not in seen_titles:
                    unique_candidates.append(c)
                    seen_titles.add(c['title'])

            final_output = []
            existing_indices = {c['index'] for c in unique_candidates if re.match(r'^\d', c['index'])}
            last_valid_main = 0

            for item in unique_candidates:
                if re.search(r'(SD\s*=|p\s*[<=]|vol\.|http|M\s*=)', item['title'], re.I): continue
                
                curr_idx = item['index']
                if re.match(r'^\d', curr_idx):
                    try:
                        main_num = int(curr_idx.split('.')[0])
                        if main_num < last_valid_main or main_num > last_valid_main + 1:
                            if last_valid_main != 0: continue
                        last_valid_main = main_num
                    except: continue

                if '.' in curr_idx:
                    parts = curr_idx.split('.')
                    for depth in range(1, len(parts)):
                        parent_idx = ".".join(parts[:depth])
                        if parent_idx not in existing_indices:
                            found_title = f"{parent_idx}."
                            search_pattern = re.compile(rf'^{re.escape(parent_idx)}\.?\s+([A-Z\u4e00-\u9fa5].*)')
                            for p_line in reversed(all_potential_lines):
                                if p_line['page'] <= item['page']:
                                    m = search_pattern.match(p_line['text'])
                                    if m:
                                        found_title = p_line['text'].split('  ')[0].strip()
                                        break
                            final_output.append({"title": found_title, "page": item['page']})
                            existing_indices.add(parent_idx)

                final_output.append({"title": item['title'], "page": item['page']})

            final_output.sort(key=lambda x: x['page'])

            # =========================================================================
            # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢æ ¸å¿ƒé€»è¾‘ï¼šæå–é‡ç‚¹ç« èŠ‚åŸæ–‡ (Reference Content) ğŸ”¥ğŸ”¥ğŸ”¥
            # =========================================================================
            processed_output = []
            total_sections = len(final_output)

            for idx, item in enumerate(final_output):
                # é»˜è®¤æ²¡æœ‰ summary
                item['summary'] = ""
                
                # 1. åˆ¤æ–­æ˜¯å¦æ˜¯é‡ç‚¹ç« èŠ‚ (Intro, Methodology, etc.)
                if TARGET_SECTIONS.search(item['title']):
                    start_page = item['page']
                    # ç¡®å®šç»“æŸé¡µç ï¼šä¸‹ä¸€ç« çš„èµ·å§‹é¡µï¼Œæˆ–è€…æ˜¯æ–‡æ¡£æœ«å°¾
                    if idx + 1 < total_sections:
                        end_page = final_output[idx+1]['page']
                    else:
                        end_page = len(pdf.pages)
                    
                    # 2. æå–è¯¥èŒƒå›´å†…çš„æ–‡æœ¬
                    extracted_text = ""
                    # é™åˆ¶æå–é¡µæ•°ï¼Œé˜²æ­¢çˆ†å†…å­˜ï¼Œæœ€å¤šæå– 3 é¡µ
                    extract_limit = min(end_page, start_page + 2) 
                    
                    for p_num in range(start_page, extract_limit + 1):
                        # pdfplumberé¡µç ä»0å¼€å§‹ï¼Œæ‰€ä»¥è¦ -1
                        if p_num <= len(pdf.pages):
                            page_text = pdf.pages[p_num-1].extract_text()
                            if page_text:
                                extracted_text += page_text + "\n"
                    
                    # 3. ç”Ÿæˆ Reference Content (è¿™é‡Œæˆ‘ä»¬ç›´æ¥æˆªå–å‰1000å­—ä½œä¸º"åŸæ–‡ç‰‡æ®µ")
                    # è¿™ä¸€æ­¥éå¸¸å…³é”®ï¼šæˆ‘ä»¬ä¸éœ€è¦ AI å®æ—¶æ€»ç»“ï¼Œç›´æ¥æŠŠåŸæ–‡å–‚ç»™"è„‘æ´å·¥ç¨‹å¸ˆ"æ•ˆæœæ›´å¥½
                    # å› ä¸ºè„‘æ´å·¥ç¨‹å¸ˆçš„ Prompt å·²ç»å…·å¤‡äº†å¤„ç†åŸæ–‡çš„èƒ½åŠ›
                    clean_text = extracted_text.replace('\n', ' ').strip()
                    item['summary'] = clean_text[:1200]  # é™åˆ¶é•¿åº¦ï¼Œå­˜å…¥æ•°æ®åº“

                processed_output.append(item)

            print(json.dumps(processed_output, ensure_ascii=False))

    except Exception:
        print("[]")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("[]")
    else:
        smart_logic_parse(sys.argv[1])